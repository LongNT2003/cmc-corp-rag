{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from FlagEmbedding import FlagReranker\n",
    "import heapq\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_qdrant import QdrantVectorStore\n",
    "from langchain_core.runnables import RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LLMHandler:\n",
    "    def __init__(self, model_name: str, gemini_key: str):\n",
    "        self.api_key = gemini_key\n",
    "        self.llm = ChatGoogleGenerativeAI(model=model_name, api_key=self.api_key)\n",
    "    \n",
    "    def get_llm(self):\n",
    "        return self.llm\n",
    "class VectorDatabase:\n",
    "    def __init__(self, model_name: str, collection_name: str, db_path: str):\n",
    "        self.embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "        self.collection_name = collection_name\n",
    "        self.db_path = db_path\n",
    "        self.db = self.load_db()\n",
    "        \n",
    "    def load_db(self):\n",
    "        return QdrantVectorStore.from_existing_collection(\n",
    "            embedding=self.embeddings,\n",
    "            collection_name=self.collection_name,\n",
    "            path=self.db_path\n",
    "        )\n",
    "    def get_retriever(self):\n",
    "        return self.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class QuestionAnsweringChain:\n",
    "    def __init__(self, llm_handler: LLMHandler, vector_db: VectorDatabase, num_docs: int = 5, apply_rewrite: bool = False, apply_rerank: bool = False):\n",
    "        self.num_docs = num_docs\n",
    "        self.llm = llm_handler.get_llm()\n",
    "        self.db = vector_db.get_retriever()\n",
    "        self.memory = []\n",
    "        if apply_rerank:\n",
    "            self.retriever = self.db.as_retriever(search_kwargs={\"k\": num_docs * 2})\n",
    "        else:\n",
    "            self.retriever = self.db.as_retriever(search_kwargs={\"k\": num_docs})\n",
    "        self.output_parser = StrOutputParser()\n",
    "        self.prompt_template = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "            Bạn là chatbot thông minh. Dựa vào những thông tin dưới đây, nếu không có dữ liệu liên quan đến câu hỏi, hãy trả lời 'Chúng tôi không có thông tin', ngoài ra có thể có 1 số câu hỏi không cần thôn tin dưới, hãy trả lời tự nhiên:\n",
    "            {context}\n",
    "\n",
    "            Lịch sử hội thoại:\n",
    "            {chat_history}\n",
    "\n",
    "            Hãy trả lời câu hỏi sau: {question}\n",
    "            \"\"\"\n",
    "        )\n",
    "        self.reranker = FlagReranker('namdp-ptit/ViRanker', use_fp16=True)\n",
    "\n",
    "        self.chain = self.create_chain(apply_rewrite=apply_rewrite, apply_rerank=apply_rerank)\n",
    "\n",
    "    def format_docs(self, docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    def ReWrite(self, query):\n",
    "        template = f'''\n",
    "        Viết lại câu hỏi dưới đây sao cho rõ ràng, chính xác và phù hợp với ngữ cảnh tìm kiếm. Thêm vào đó, cung cấp một chút gợi ý và suy luận để tăng khả năng tìm kiếm và trả lời từ cơ sở dữ liệu. Đảm bảo câu hỏi mới vẫn giữ nguyên ý nghĩa của câu hỏi gốc.\n",
    "\n",
    "        Câu hỏi gốc: \"{query}\"\n",
    "\n",
    "        Câu hỏi viết lại:\n",
    "        '''\n",
    "        rewrite_query = self.llm.invoke(template)\n",
    "        return rewrite_query.content\n",
    "    def ReRank(self, query_docs):\n",
    "        query = query_docs['query']\n",
    "        chunks = query_docs['docs']\n",
    "        top_k = self.num_docs\n",
    "        scores = self.reranker.compute_score(\n",
    "            [[query, chunk.page_content] for chunk in chunks],\n",
    "            normalize=True\n",
    "        )\n",
    "        chunk_with_rank = [(chunks[idx], scores[idx]) for idx in range(len(chunks))]\n",
    "        top_chunks = heapq.nlargest(top_k, chunk_with_rank, key=lambda x: x[1])\n",
    "        return [chunk for chunk, score in top_chunks]\n",
    "\n",
    "    def find_neighbor(self, docs):\n",
    "        for doc in docs:\n",
    "            doc_id = doc.metadata['_id']\n",
    "            neighbor_ids = [doc_id - 2, doc_id - 1, doc_id + 1, doc_id + 2]\n",
    "            neighbors = self.db.get_by_ids(neighbor_ids)\n",
    "            neighbors.append(doc)\n",
    "            neighbors_sorted = sorted(neighbors, key=lambda x: x.metadata['_id'])\n",
    "            doc.page_content = '\\n'.join([neighbor.page_content for neighbor in neighbors_sorted])\n",
    "        return docs\n",
    "\n",
    "    def get_chat_history(self):\n",
    "        return '\\n'.join(self.memory) if self.memory else \"\"\n",
    "\n",
    "\n",
    "    def create_chain(self, apply_rewrite: bool = False, apply_rerank: bool = False):\n",
    "        retriever_handler = self.retriever\n",
    "        if apply_rewrite:\n",
    "            pre_retriever = self.ReWrite\n",
    "        else:\n",
    "            pre_retriever = RunnablePassthrough()\n",
    "        if apply_rerank:\n",
    "            retriever_handler = RunnableParallel(\n",
    "                {'docs': retriever_handler, 'query': RunnablePassthrough()}\n",
    "            )\n",
    "            retriever_handler = retriever_handler | self.ReRank\n",
    "\n",
    "            \n",
    "        retriever_handler = retriever_handler | self.find_neighbor | self.format_docs\n",
    "        chat_history_handler = RunnableLambda(lambda x: self.get_chat_history())\n",
    "        setup_and_retrieval = RunnableParallel(\n",
    "            {\"context\": retriever_handler, \"question\": RunnablePassthrough(), 'chat_history': chat_history_handler}\n",
    "        )\n",
    "        chain = pre_retriever | setup_and_retrieval | self.prompt_template | self.llm | self.output_parser\n",
    "        return chain\n",
    "\n",
    "    def run(self, question: str):\n",
    "        # Lưu lịch sử hội thoại\n",
    "        self.memory.append(f'người dùng: {question}')\n",
    "        response = self.chain.invoke(question)\n",
    "\n",
    "        # Cập nhật phản hồi vào lịch sử hội thoại\n",
    "        self.memory.append(f'chatbot: {response}')\n",
    "        if len(self.memory) > 3:\n",
    "            self.memory.pop(0)\n",
    "            self.memory.pop(0)\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load llm\n",
    "load_dotenv()\n",
    "gemini_key=os.getenv('gemini_key')\n",
    "# Initialize the classes\n",
    "llm_handler = LLMHandler(model_name=\"gemini-1.5-flash\", gemini_key=gemini_key)\n",
    "vector_db = VectorDatabase(\n",
    "    model_name=\"hiieu/halong_embedding\",\n",
    "    collection_name='cmc_corp_full_web',\n",
    "    db_path='db_qdrant'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = QuestionAnsweringChain(llm_handler, vector_db,num_docs=5, apply_rerank=False, apply_rewrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "while 1:\n",
    "    print('cuộc hội thoại, nhắn exit nếu muốn thoát.')\n",
    "    question=input('user: ')\n",
    "    if question.lower()=='exit':\n",
    "        break\n",
    "    # Run the chain with a question\n",
    "    result = qa_chain.run(question)\n",
    "    print('response: \\n'+result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
